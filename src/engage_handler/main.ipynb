{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Download pre-trained word2vec model.\n",
    "!python -m gensim.downloader --download word2vec-google-news-300"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T03:47:10.421326Z",
     "start_time": "2024-03-19T03:47:09.789682Z"
    }
   },
   "id": "3d27b8662357590e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from collections import deque, namedtuple\n",
    "import socket\n",
    "from threading import Thread\n",
    "from math import log10\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from action_matrix import ACTION_ID2NAME, ACTION_LEN, ACTIVITY_IDF2ID, ACTIVITY_IDF2NAME, ACTIVITY_LEN, MAPPING_ACTIVITY2ACTION"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:45:33.658539Z",
     "start_time": "2024-04-08T19:45:33.656537Z"
    }
   },
   "id": "d01806251a0b4323",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:45:35.464117Z",
     "start_time": "2024-04-08T19:45:35.460966Z"
    }
   },
   "id": "a1de7d4ec44a49a",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300', binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:45:46.509868Z",
     "start_time": "2024-04-08T19:45:37.246606Z"
    }
   },
   "id": "cc0d748e1fbc2aff",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "SERVER_ADDRESS = '0.0.0.0'\n",
    "SERVER_PORT = 6416\n",
    "LOGIN_STATE_ID2NAME = ('Trying', 'Normal User', 'Root User')\n",
    "ATTRIBUTE_NAME = ('exec', 'mail_sender', 'mail_receiver', 'username', 'password', 'interface', 'domain', 'ip', 'port', 'path')\n",
    "ATTRIBUTE_DEFAULT = ('', '', '', '', '', 'enp1s0', 'localhost', '127.0.0.1', '', '')\n",
    "NUM_EPISODES = 1000\n",
    "TARGET_UPDATE_STEP = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:45:48.753594Z",
     "start_time": "2024-04-08T19:45:48.745169Z"
    }
   },
   "id": "1c83046115d67394",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:45:50.189199Z",
     "start_time": "2024-04-08T19:45:50.137078Z"
    }
   },
   "source": [
    "# Define the environment.\n",
    "class CustomEnvironment:\n",
    "    @staticmethod\n",
    "    def reset() -> Tuple[int, List[str], List[int]]:\n",
    "        \n",
    "        return 0, list(ATTRIBUTE_DEFAULT), []\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def action_evaluation(suggested_activities: List[int], encoded_action: int) -> float:\n",
    "        \n",
    "        suggested_activities_id = [ACTIVITY_IDF2ID.index(f'EAC{act:04d}') for act in suggested_activities if act in ACTIVITY_IDF2ID]\n",
    "        suggested_actions = []\n",
    "        \n",
    "        for act_id in suggested_activities_id:\n",
    "            if 1 <= act_id <= ACTIVITY_LEN:\n",
    "                login_state, action_id = MAPPING_ACTIVITY2ACTION[str(act_id)]\n",
    "                encoded_action = login_state * 100 + action_id\n",
    "                \n",
    "                if encoded_action not in suggested_actions:\n",
    "                    suggested_actions.append(encoded_action)\n",
    "              \n",
    "        if encoded_action in suggested_actions:  \n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.previous_state = self.reset()\n",
    "        self.current_state = self.reset()\n",
    "        self.done = False\n",
    "        self.step_counter = 0\n",
    "\n",
    "\n",
    "    def step(self, action: int, state: Tuple[int, List[str], List[int]]) -> float:\n",
    "        \n",
    "        self.previous_state = self.current_state\n",
    "        self.current_state = state\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        reward = self.action_evaluation(self.previous_state[2], (self.previous_state[0] * 100 + action)) + log10(self.step_counter)\n",
    "        \n",
    "        if state[0] == -1:\n",
    "            self.done = True\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "# Define the neural network architecture.\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the double DQN with replay buffer.\n",
    "class DoubleDQN:\n",
    "    transition = namedtuple('transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def __init__(self, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.01, batch_size=32, buffer_size=100):\n",
    "        \n",
    "        self.input_size = 317\n",
    "        self.output_size = 16\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.policy_net = DQN(self.input_size, self.output_size)\n",
    "        self.target_net = DQN(self.input_size, self.output_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> int:\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.output_size - 1) + 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.argmax().item() + 1\n",
    "\n",
    "\n",
    "    def store_transition(self, state: torch.Tensor, action: int, reward: float, next_state: torch.Tensor, done: bool) -> None:\n",
    "        \n",
    "        self.replay_buffer.append(DoubleDQN.transition(state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def sample_batch(self) -> List[DoubleDQN.transition]:\n",
    "        \n",
    "        transitions = random.sample(self.replay_buffer, self.batch_size)\n",
    "        \n",
    "        return transitions\n",
    "        \n",
    "        # batch = DoubleDQN.transition(*zip(*transitions))\n",
    "        # states = torch.tensor(batch.state, dtype=torch.float32)\n",
    "        # actions = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1)\n",
    "        # rewards = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1)\n",
    "        # next_states = torch.tensor(batch.next_state, dtype=torch.float32)\n",
    "        # dones = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1)\n",
    "        # \n",
    "        # return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "    def train(self, state: torch.Tensor, action: int, reward: float, next_state: torch.Tensor, done: bool) -> None:\n",
    "        \n",
    "        self.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        for state, action, reward, next_state, done in self.sample_batch():\n",
    "            try:\n",
    "                q_values = self.policy_net(state)\n",
    "            except:\n",
    "                print('State =', state)\n",
    "                \n",
    "            next_q_value = self.target_net(next_state).detach()[self.policy_net(next_state).argmax()]\n",
    "    \n",
    "            q_value = q_values[action]\n",
    "            expected_q_value = reward + (1 - done) * self.gamma * next_q_value\n",
    "    \n",
    "            loss = self.criterion(q_value, expected_q_value)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        \n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        \n",
    "class EngageHandler:\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode_state(encoded_state: bytes) -> Tuple[int, List[str], List[int]]:\n",
    "        \n",
    "        state: List[str] = json.loads(encoded_state.decode())['state']\n",
    "                \n",
    "        login_state = int(state[0])\n",
    "        attr = state[1:11]\n",
    "        \n",
    "        if len(state) > 11:\n",
    "            suggested_activities = [int(act.replace('EAC', '')) for act in state[11:]]\n",
    "        else:\n",
    "            suggested_activities = []\n",
    "\n",
    "        return login_state, attr, suggested_activities\n",
    "        \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.keep_running = True\n",
    "        self.server: Union[socket.socket, None] = None\n",
    "        self.clients: List[Thread] = []\n",
    "        self.states: List[Union[Tuple[int, List[str], List[int]], None]] = []\n",
    "        self.actions: List[int] = []\n",
    "        \n",
    "        self.init_server()\n",
    "        \n",
    "        \n",
    "    def init_server(self):\n",
    "        \n",
    "        self.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.server.bind((SERVER_ADDRESS, SERVER_PORT))\n",
    "        self.server.listen(5)\n",
    "        \n",
    "        print(f'Server is listening on \"{SERVER_ADDRESS}:{SERVER_PORT}\".')\n",
    "        \n",
    "        while self.keep_running:\n",
    "            try:\n",
    "                con, client = self.server.accept()\n",
    "            except KeyboardInterrupt:\n",
    "                self.close_server()\n",
    "                break\n",
    "            \n",
    "            t = Thread(target=self.handle_con, args=(client, con))\n",
    "            t.start()\n",
    "            \n",
    "            self.clients.append(t)\n",
    "            \n",
    "            \n",
    "    def close_server(self):\n",
    "        \n",
    "        self.keep_running = False\n",
    "        \n",
    "        if self.server is not None:\n",
    "            self.server.close()\n",
    "            \n",
    "        print('Server closed.')\n",
    "        \n",
    "        \n",
    "    def handle_con(self, client: Tuple[str, int], con: socket.socket):\n",
    "        \n",
    "        client_id = len(self.states)\n",
    "        self.states.append(None)\n",
    "        self.actions.append(-1)\n",
    "        \n",
    "        print(f'[{client_id}] Accepted connection from \"{client[0]}:{client[1]}\". Client id is {client_id}.')\n",
    "        \n",
    "        env = CustomEnvironment()\n",
    "        agent = DoubleDQN()\n",
    "        total_reward = 0\n",
    "        PREDEFINED_ACTIONS = [[1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 14, 14, 5, 11], [11, 11], [12, 13, 13, 9, 7, 8], [6, 10, 15, 15, 16, 16, 16, 16, 16, 16], []]\n",
    "        \n",
    "        with con:\n",
    "            con.settimeout(1)\n",
    "            \n",
    "            while self.keep_running and not env.done:\n",
    "                action = agent.select_action(state_to_tensor(env.current_state))\n",
    "                \n",
    "                # if len(PREDEFINED_ACTIONS[client_id]) > 0:\n",
    "                #     action = PREDEFINED_ACTIONS[client_id].pop(0)\n",
    "                \n",
    "                con.sendall(str(action).encode())\n",
    "                print(f'[{client_id}] Selected action is {action} \"{ACTION_ID2NAME[action]}\".')\n",
    "                \n",
    "                while self.keep_running:\n",
    "                    try:\n",
    "                        encoded_state = con.recv(1024)\n",
    "                    except ConnectionResetError:\n",
    "                        encoded_state = ''\n",
    "                        break\n",
    "                    except TimeoutError:\n",
    "                        pass\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                if len(encoded_state) == 0:\n",
    "                    break\n",
    "                \n",
    "                if self.keep_running:\n",
    "                    if encoded_state:\n",
    "                        next_state = EngageHandler.decode_state(encoded_state)\n",
    "                    else:\n",
    "                        next_state = (-1, list(ATTRIBUTE_DEFAULT), [])\n",
    "                        \n",
    "                    print(f'[{client_id}] Next state is {next_state}.')\n",
    "                    \n",
    "                    reward = env.step(action - 1, next_state)\n",
    "                    total_reward += reward\n",
    "                    \n",
    "                    print(f'[{client_id}] [{env.step_counter}] From state {env.previous_state}, do action {action}, to state {env.current_state}. (reward={reward}, total={total_reward})')\n",
    "                    \n",
    "                    agent.train(state_to_tensor(env.previous_state), action, reward, state_to_tensor(env.current_state), env.done)\n",
    "            \n",
    "                    if env.step_counter % TARGET_UPDATE_STEP == 0:\n",
    "                        agent.update_target_network()\n",
    "                \n",
    "        print(f'[{client_id}] Disconnected from {client[0]}:{client[1]} .')\n",
    "\n",
    "\n",
    "def encode_login_state(state: int) -> np.ndarray:\n",
    "    \n",
    "    # Convert integer to one-hot encoding.\n",
    "    return np.eye(3)[state]\n",
    "\n",
    "\n",
    "def embed_attributes(attributes: List[str]) -> np.ndarray:\n",
    "    \n",
    "    embedded_attributes = []\n",
    "    \n",
    "    for attr in attributes:\n",
    "        # Assuming you have a trained Word2Vec model\n",
    "        embedded_attr = [word2vec_model[word] for word in attr.split() if word in word2vec_model]\n",
    "        embedded_attributes += embedded_attr\n",
    "        \n",
    "    return np.array(embedded_attributes).flatten()\n",
    "\n",
    "\n",
    "def encode_suggested_activities(activities: List[int]) -> np.ndarray:\n",
    "    \n",
    "    # Convert activity IDFs to Multi-hot encoding.\n",
    "    encoded_activities = np.zeros(ACTIVITY_LEN)\n",
    "    \n",
    "    for activity in activities:\n",
    "        if 1 <= activity <= ACTIVITY_LEN:\n",
    "            encoded_activities[activity - 1] = 1\n",
    "    \n",
    "    return encoded_activities\n",
    "\n",
    "\n",
    "def state_to_tensor(state: Tuple[int, List[str], List[int]]) -> torch.Tensor:\n",
    "    \n",
    "    # Return an one dimension tensor: 3 + 30 * 10 + 14 = 317.\n",
    "    \n",
    "    login_state, attributes, suggested_activities = state\n",
    "    login_state_tensor = torch.tensor(encode_login_state(login_state), dtype=torch.float32)\n",
    "    attribute_tensor = torch.tensor(embed_attributes(attributes), dtype=torch.float32)\n",
    "    suggested_activities_tensor = torch.tensor(encode_suggested_activities(suggested_activities), dtype=torch.float32)\n",
    "    \n",
    "    return torch.cat((login_state_tensor, attribute_tensor, suggested_activities_tensor))"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eh = EngageHandler()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95033372a9380b6",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
